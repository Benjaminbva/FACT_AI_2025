{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def boundary_margin(embeddings_c1, embeddings_c2):\n",
    "    \"\"\"\n",
    "    Compute the boundary margin.\n",
    "\n",
    "    Args:\n",
    "    - embeddings_c1 (torch.Tensor): Embeddings of class c1 graphs.\n",
    "    - embeddings_c2 (torch.Tensor): Embeddings of boundary graphs between class c1 and c2.\n",
    "\n",
    "    Returns:\n",
    "    - margin (float): The boundary margin.\n",
    "\n",
    "    \"\"\"\n",
    "    embeddings_c1=torch.cat(embeddings_c1,dim=0)\n",
    "    embeddings_c2=torch.cat(embeddings_c2,dim=0)\n",
    "    distances = torch.norm(embeddings_c1 - embeddings_c2, dim=1)\n",
    "    margin = torch.min(distances).item()\n",
    "    return margin\n",
    "\n",
    "def boundary_thickness(embeddings_c1, embeddings_c1_c2, model, c1, c2, gamma=0.75, num_points=100):\n",
    "    thickness_values = []\n",
    "\n",
    "    for emb_c1, emb_c1_c2 in zip(embeddings_c1, embeddings_c1_c2):\n",
    "        t_values = torch.linspace(0, 1, num_points)\n",
    "        h_t = (1 - t_values).unsqueeze(1) * emb_c1 + t_values.unsqueeze(1) * emb_c1_c2\n",
    "        #print(model(h_t).size())\n",
    "\n",
    "        # Compute the logits\n",
    "        logits_h_t = model(h_t)  # Assuming model is your classifier\n",
    "        probs_h_t = F.softmax(logits_h_t, dim=1)\n",
    "\n",
    "        # Compute the integrand\n",
    "        integrand = (gamma > (probs_h_t[:, c1] - probs_h_t[:, c2])).float()\n",
    "\n",
    "        # Approximate the integral using the trapezoidal rule\n",
    "        integral = torch.trapz(integrand, t_values)\n",
    "\n",
    "        # Compute the thickness value\n",
    "        thickness_value = (emb_c1 - emb_c1_c2).norm() * integral.mean()\n",
    "        thickness_values.append(thickness_value.item())\n",
    "\n",
    "    return sum(thickness_values) / len(thickness_values)\n",
    "\n",
    "def boundary_complexity(embeddings, D, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Compute the boundary complexity.\n",
    "\n",
    "    Args:\n",
    "    - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
    "    - D (int): Dimensionality of the embeddings.\n",
    "    - epsilon (float): Small value added to eigenvalues to prevent log(0).\n",
    "\n",
    "    Returns:\n",
    "    - complexity (float): The boundary complexity.\n",
    "    \"\"\"\n",
    "    # Flatten and concatenate embeddings\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    # Compute the covariance matrix of the embeddings\n",
    "    covariance_matrix = torch.cov(embeddings.T)\n",
    "\n",
    "    # Add a small value to the diagonal for regularization\n",
    "    covariance_matrix += epsilon * torch.eye(covariance_matrix.size(0))\n",
    "\n",
    "    # Compute the eigenvalues of the covariance matrix\n",
    "    eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
    "\n",
    "    # Clamp eigenvalues to avoid very small negative values due to numerical errors\n",
    "    eigenvalues = torch.clamp(eigenvalues, min=epsilon)\n",
    "\n",
    "    # Normalize the eigenvalues\n",
    "    eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
    "\n",
    "    # Compute the entropy of the normalized eigenvalues\n",
    "    entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + epsilon))\n",
    "\n",
    "    # Normalize the entropy by dividing it by log(D)\n",
    "    complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
    "\n",
    "    return complexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m boundaryembeddings\u001b[38;5;241m=\u001b[39m\u001b[43membeddings\u001b[49m\n\u001b[1;32m      2\u001b[0m latent_data\u001b[38;5;241m=\u001b[39mlatent_data2\n\u001b[1;32m      3\u001b[0m margin\u001b[38;5;241m=\u001b[39mboundary_margin(boundaryembeddings[:\u001b[38;5;28mlen\u001b[39m(latent_data)],latent_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# weet alleen niet wat die embeddings / laten_data2 precies is\n",
    "# komt uit https://github.com/amisayan/Gen-GraphEx-Review/blob/main/Gen_GraphEx_MUTAG.ipynb\n",
    "\n",
    "boundaryembeddings=embeddings\n",
    "latent_data=latent_data2\n",
    "margin=boundary_margin(boundaryembeddings[:len(latent_data)],latent_data)\n",
    "print(margin)\n",
    "thickness=boundary_thickness(boundaryembeddings[:len(latent_data)],latent_data,model.classifier,1,0)\n",
    "print(thickness)\n",
    "complexity=boundary_complexity(boundaryembeddings[:len(latent_data)],64)\n",
    "print(complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnboundary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
